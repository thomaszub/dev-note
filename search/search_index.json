{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Dev Note Notes about various topics in software development.","title":"Introduction"},{"location":"index.html#dev-note","text":"Notes about various topics in software development.","title":"Dev Note"},{"location":"agile_methods/index.html","text":"Agile methods This note contains informations about agile methods.","title":"Agile methods"},{"location":"agile_methods/index.html#agile-methods","text":"This note contains informations about agile methods.","title":"Agile methods"},{"location":"agile_methods/invest.html","text":"INVEST The INVEST mnemonic stands for six characteristics for a work item. I ndependent N egotiable V aluable E stimable S mall T estable","title":"INVEST"},{"location":"agile_methods/invest.html#invest","text":"The INVEST mnemonic stands for six characteristics for a work item. I ndependent N egotiable V aluable E stimable S mall T estable","title":"INVEST"},{"location":"concepts/index.html","text":"Concepts This note contains informations about concepts like domain-driven design.","title":"Concepts"},{"location":"concepts/index.html#concepts","text":"This note contains informations about concepts like domain-driven design.","title":"Concepts"},{"location":"concepts/clean_code/index.html","text":"Clean Code This note contains informations about clean code. Open Closed Principle Avoid switch statements, use polymorphism Command and query separation Function returning void has a side effect (command), function returning a value should have no side effects (query)","title":"Clean code"},{"location":"concepts/clean_code/index.html#clean-code","text":"This note contains informations about clean code.","title":"Clean Code"},{"location":"concepts/clean_code/index.html#open-closed-principle","text":"Avoid switch statements, use polymorphism","title":"Open Closed Principle"},{"location":"concepts/clean_code/index.html#command-and-query-separation","text":"Function returning void has a side effect (command), function returning a value should have no side effects (query)","title":"Command and query separation"},{"location":"concepts/domain_driven_design/index.html","text":"Domain-driven design This note contains informations about domain-driven design. Bounded context The bounded context is the context in which a model applies. It explicitly set boundaries to other bounded contexts. Example: Customer An example of different bounded contexts with a customer model is: Marketing -> Social interest, likes, needs Invoice -> Address, payment method Support -> Tickets All these three bounded contexts share the customer as a boundary.","title":"Domain driven design"},{"location":"concepts/domain_driven_design/index.html#domain-driven-design","text":"This note contains informations about domain-driven design.","title":"Domain-driven design"},{"location":"concepts/domain_driven_design/index.html#bounded-context","text":"The bounded context is the context in which a model applies. It explicitly set boundaries to other bounded contexts.","title":"Bounded context"},{"location":"concepts/domain_driven_design/index.html#example-customer","text":"An example of different bounded contexts with a customer model is: Marketing -> Social interest, likes, needs Invoice -> Address, payment method Support -> Tickets All these three bounded contexts share the customer as a boundary.","title":"Example: Customer"},{"location":"concepts/domain_driven_design/entities.html","text":"Entities Late vs early identity generation","title":"Entities"},{"location":"concepts/domain_driven_design/entities.html#entities","text":"Late vs early identity generation","title":"Entities"},{"location":"concepts/event_driven_architecture/index.html","text":"Event-driven architecture Event notification Pro Decoupling sender and receiver Contra No statement of overall behavior Event-carried state transfer Pro Further decoupling Reduced load on sender Contra Replicated data, eventual consistency Event sourcing Pro TODO Contra TODO CQRS Pro TODO Contra TODO","title":"Event driven architecture"},{"location":"concepts/event_driven_architecture/index.html#event-driven-architecture","text":"","title":"Event-driven architecture"},{"location":"concepts/event_driven_architecture/index.html#event-notification","text":"","title":"Event notification"},{"location":"concepts/event_driven_architecture/index.html#pro","text":"Decoupling sender and receiver","title":"Pro"},{"location":"concepts/event_driven_architecture/index.html#contra","text":"No statement of overall behavior","title":"Contra"},{"location":"concepts/event_driven_architecture/index.html#event-carried-state-transfer","text":"","title":"Event-carried state transfer"},{"location":"concepts/event_driven_architecture/index.html#pro_1","text":"Further decoupling Reduced load on sender","title":"Pro"},{"location":"concepts/event_driven_architecture/index.html#contra_1","text":"Replicated data, eventual consistency","title":"Contra"},{"location":"concepts/event_driven_architecture/index.html#event-sourcing","text":"","title":"Event sourcing"},{"location":"concepts/event_driven_architecture/index.html#pro_2","text":"TODO","title":"Pro"},{"location":"concepts/event_driven_architecture/index.html#contra_2","text":"TODO","title":"Contra"},{"location":"concepts/event_driven_architecture/index.html#cqrs","text":"","title":"CQRS"},{"location":"concepts/event_driven_architecture/index.html#pro_3","text":"TODO","title":"Pro"},{"location":"concepts/event_driven_architecture/index.html#contra_3","text":"TODO","title":"Contra"},{"location":"design_pattern/index.html","text":"Design pattern This note contains informations about design patterns, mainly from the book \"Design Patterns\" from the Gang of Four.","title":"Design pattern"},{"location":"design_pattern/index.html#design-pattern","text":"This note contains informations about design patterns, mainly from the book \"Design Patterns\" from the Gang of Four.","title":"Design pattern"},{"location":"design_pattern/behavioral/index.html","text":"Behavioral Behavioral design patterns describe the communications between objects in a system.","title":"Behavioral"},{"location":"design_pattern/behavioral/index.html#behavioral","text":"Behavioral design patterns describe the communications between objects in a system.","title":"Behavioral"},{"location":"design_pattern/behavioral/observer.html","text":"Observer The observer is publish-subscribe-pattern typically found in event-driven systems. The observable maintains a list of observers and notifies them on events like changes in state. sequenceDiagram participant Observable participant Observer1 participant Observer2 Observer1->>Observable: register Observer2->>Observable: register Note over Observable: Event occurs in Observable Observable->>Observer1: update(Event) Observable->>Observer2: update(Event) Weak reference In the case that the observable holds a strong reference to an observer, the observer would not be cleaned up by e.g garbage collection or reference counting if its life cycle ends. This can be solved by weak references.","title":"Observer"},{"location":"design_pattern/behavioral/observer.html#observer","text":"The observer is publish-subscribe-pattern typically found in event-driven systems. The observable maintains a list of observers and notifies them on events like changes in state. sequenceDiagram participant Observable participant Observer1 participant Observer2 Observer1->>Observable: register Observer2->>Observable: register Note over Observable: Event occurs in Observable Observable->>Observer1: update(Event) Observable->>Observer2: update(Event)","title":"Observer"},{"location":"design_pattern/behavioral/observer.html#weak-reference","text":"In the case that the observable holds a strong reference to an observer, the observer would not be cleaned up by e.g garbage collection or reference counting if its life cycle ends. This can be solved by weak references.","title":"Weak reference"},{"location":"design_pattern/behavioral/visitor.html","text":"Visitor The visitor pattern separates algorithms from the object structures they work on. If supported, this can be done by double dispatch or reflection. Java In Java this can be done with the following interfaces including return value and exception propagation: public interface Visitor < R , E extends Throwable > { R visit ( ConcreteNode1 node ) throws E ; R visit ( ConcreteNode2 node ) throws E ; ... } public interface Node { < R , E extends Throwable > R accept ( Visitor < R , E > visitor ) throws E ; } //Example of concrete implementation public class ConcreteNode1 implements Node { public < R , E extends Throwable > R accept ( Visitor < R , E > visitor ) throws E { return visitor . visit ( this ); } ... } The implementation for a concrete node looks always the same and allows passing Void for R if no returned value is needed.","title":"Visitor"},{"location":"design_pattern/behavioral/visitor.html#visitor","text":"The visitor pattern separates algorithms from the object structures they work on. If supported, this can be done by double dispatch or reflection.","title":"Visitor"},{"location":"design_pattern/behavioral/visitor.html#java","text":"In Java this can be done with the following interfaces including return value and exception propagation: public interface Visitor < R , E extends Throwable > { R visit ( ConcreteNode1 node ) throws E ; R visit ( ConcreteNode2 node ) throws E ; ... } public interface Node { < R , E extends Throwable > R accept ( Visitor < R , E > visitor ) throws E ; } //Example of concrete implementation public class ConcreteNode1 implements Node { public < R , E extends Throwable > R accept ( Visitor < R , E > visitor ) throws E { return visitor . visit ( this ); } ... } The implementation for a concrete node looks always the same and allows passing Void for R if no returned value is needed.","title":"Java"},{"location":"frameworks_and_tools/index.html","text":"Frameworks and tools This note contains informations about frameworks and tools.","title":"Frameworks and tools"},{"location":"frameworks_and_tools/index.html#frameworks-and-tools","text":"This note contains informations about frameworks and tools.","title":"Frameworks and tools"},{"location":"frameworks_and_tools/peotry.html","text":"Poetry Poetry is a Python tool for package and dependency management. Configuration To create the virtual enviroments in the project and not in a global directory, the following command can be used: poetry config virtualenvs.in-project true","title":"Poetry"},{"location":"frameworks_and_tools/peotry.html#poetry","text":"Poetry is a Python tool for package and dependency management.","title":"Poetry"},{"location":"frameworks_and_tools/peotry.html#configuration","text":"To create the virtual enviroments in the project and not in a global directory, the following command can be used: poetry config virtualenvs.in-project true","title":"Configuration"},{"location":"machine_learning/index.html","text":"Machine learning This note is about machine learning.","title":"Machine learning"},{"location":"machine_learning/index.html#machine-learning","text":"This note is about machine learning.","title":"Machine learning"},{"location":"machine_learning/activation_functions.html","text":"Activation functions Sigmoid / Logistic The Sigmoid activation function is defined by \\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\] As the Sigmoid function has the bounds [0, 1], it is suited for multi-label classification as the element results can be trained as probabilities for a certain label to be true or not. Softmax The Softmax activation function is defined by \\[ \\sigma_j(x) = \\frac{e^{x_j}}{\\sum_j{e^{x_j}}} \\] In comparison to most activation functions, the sum of all element-wise results is constrained, precisely speaking: \\[ \\sum_j \\sigma_j(x) = 1 \\] Therefore the Softmax function is suited for multi-class classification as the element results can be trained as probabilities for a certain class in one category to be true or not.","title":"Activation functions"},{"location":"machine_learning/activation_functions.html#activation-functions","text":"","title":"Activation functions"},{"location":"machine_learning/activation_functions.html#sigmoid-logistic","text":"The Sigmoid activation function is defined by \\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\] As the Sigmoid function has the bounds [0, 1], it is suited for multi-label classification as the element results can be trained as probabilities for a certain label to be true or not.","title":"Sigmoid / Logistic"},{"location":"machine_learning/activation_functions.html#softmax","text":"The Softmax activation function is defined by \\[ \\sigma_j(x) = \\frac{e^{x_j}}{\\sum_j{e^{x_j}}} \\] In comparison to most activation functions, the sum of all element-wise results is constrained, precisely speaking: \\[ \\sum_j \\sigma_j(x) = 1 \\] Therefore the Softmax function is suited for multi-class classification as the element results can be trained as probabilities for a certain class in one category to be true or not.","title":"Softmax"},{"location":"machine_learning/data_preprocessing.html","text":"Data preprocessing Label encoder Python: from sklearn.preprocessing import LabelEncoder One hot encoder Python: from sklearn.preprocessing import OneHotEncoder Column transformer Scikit-learn contains a class for applying multiple transformations to a dataset including dropping columns: Python: from sklearn.compose import ColumnTransformer Note: Does currently not work with a LabelEncoder. Image data Python: from keras.preprocessing.image import ImageDataGenerator","title":"Data preprocessing"},{"location":"machine_learning/data_preprocessing.html#data-preprocessing","text":"","title":"Data preprocessing"},{"location":"machine_learning/data_preprocessing.html#label-encoder","text":"Python: from sklearn.preprocessing import LabelEncoder","title":"Label encoder"},{"location":"machine_learning/data_preprocessing.html#one-hot-encoder","text":"Python: from sklearn.preprocessing import OneHotEncoder","title":"One hot encoder"},{"location":"machine_learning/data_preprocessing.html#column-transformer","text":"Scikit-learn contains a class for applying multiple transformations to a dataset including dropping columns: Python: from sklearn.compose import ColumnTransformer Note: Does currently not work with a LabelEncoder.","title":"Column transformer"},{"location":"machine_learning/data_preprocessing.html#image-data","text":"Python: from keras.preprocessing.image import ImageDataGenerator","title":"Image data"},{"location":"machine_learning/feature_scaling.html","text":"Feature scaling Feature scaling is a data preprocessing step to ease training of neural networks by reducing the numeric scale of different features to a uniform scale. Standardisation Standardisation transforms the feature distribution of data to zero mean and unit variance: \\[ z = \\frac{x - \\operatorname{mean}(x)}{\\operatorname{std}(x)} \\] Python: from sklearn.preprocessing import StandardScaler Normalisation Normalisation transforms the feature distribution to the interval \\([0,1]\\) : \\[ z = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\] Python: from sklearn.preprocessing import MinMaxScaler","title":"Feature scaling"},{"location":"machine_learning/feature_scaling.html#feature-scaling","text":"Feature scaling is a data preprocessing step to ease training of neural networks by reducing the numeric scale of different features to a uniform scale.","title":"Feature scaling"},{"location":"machine_learning/feature_scaling.html#standardisation","text":"Standardisation transforms the feature distribution of data to zero mean and unit variance: \\[ z = \\frac{x - \\operatorname{mean}(x)}{\\operatorname{std}(x)} \\] Python: from sklearn.preprocessing import StandardScaler","title":"Standardisation"},{"location":"machine_learning/feature_scaling.html#normalisation","text":"Normalisation transforms the feature distribution to the interval \\([0,1]\\) : \\[ z = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} \\] Python: from sklearn.preprocessing import MinMaxScaler","title":"Normalisation"},{"location":"machine_learning/loss_functions.html","text":"Loss functions This note is about loss functions used in training neural networks. Extensions The folowing extension can be added to induce effects in minimizing the loss function. Regularization Regularization penalizes high values of the parameters in a neural network and helps against overfitting. The corresponding loss extension term has the form \\[ L_p(\\Theta) = \\lambda \\|\\Theta\\|^p \\] with \\( \\lambda \\) the regularization factor, \\( \\Theta \\) the training parameters and \\( p \\in \\mathbb{N} \\) , typically with \\( p = 2 \\) (L2-Norm). Momentum To mitigate oscillations in the training of the parameters, a momentum extension can be used. This term is added to the update of the parameters in gradient descent \\[ \\Theta^{(t+1)} = \\Theta^{(t)} + \\Delta \\Theta_{GD}^{(t)} + \\nu \\Delta \\Theta^{(t-1)} = \\Theta^{(t)} + \\Delta \\Theta^{(t)} \\] with \\( t \\) the update step, \\( \\Theta \\) the training parameters and \\( \\nu \\) the momentum factor. Cross entropy For classification problems the output of the network can be choosen to be a vector of probabilities \\( q(y) \\) for the different classes \\( y \\in Y \\) . With the expected result \\( p(y) = \\delta_{y,\\hat{y}} \\) and the correct class \\( \\hat{y} \\) , the cross entropy can be used: \\[ H(p, q) = - \\sum_{y \\in Y}{p(y) \\log(q(y))} \\]","title":"Loss functions"},{"location":"machine_learning/loss_functions.html#loss-functions","text":"This note is about loss functions used in training neural networks.","title":"Loss functions"},{"location":"machine_learning/loss_functions.html#extensions","text":"The folowing extension can be added to induce effects in minimizing the loss function.","title":"Extensions"},{"location":"machine_learning/loss_functions.html#regularization","text":"Regularization penalizes high values of the parameters in a neural network and helps against overfitting. The corresponding loss extension term has the form \\[ L_p(\\Theta) = \\lambda \\|\\Theta\\|^p \\] with \\( \\lambda \\) the regularization factor, \\( \\Theta \\) the training parameters and \\( p \\in \\mathbb{N} \\) , typically with \\( p = 2 \\) (L2-Norm).","title":"Regularization"},{"location":"machine_learning/loss_functions.html#momentum","text":"To mitigate oscillations in the training of the parameters, a momentum extension can be used. This term is added to the update of the parameters in gradient descent \\[ \\Theta^{(t+1)} = \\Theta^{(t)} + \\Delta \\Theta_{GD}^{(t)} + \\nu \\Delta \\Theta^{(t-1)} = \\Theta^{(t)} + \\Delta \\Theta^{(t)} \\] with \\( t \\) the update step, \\( \\Theta \\) the training parameters and \\( \\nu \\) the momentum factor.","title":"Momentum"},{"location":"machine_learning/loss_functions.html#cross-entropy","text":"For classification problems the output of the network can be choosen to be a vector of probabilities \\( q(y) \\) for the different classes \\( y \\in Y \\) . With the expected result \\( p(y) = \\delta_{y,\\hat{y}} \\) and the correct class \\( \\hat{y} \\) , the cross entropy can be used: \\[ H(p, q) = - \\sum_{y \\in Y}{p(y) \\log(q(y))} \\]","title":"Cross entropy"},{"location":"machine_learning/artifical_neural_networks/index.html","text":"Artificial neural networks This note is about artificial neural networks (ANN). These networks belong to supervised learning and are used in the analysis and prediction of classification and regression problems.","title":"Artifical neural networks"},{"location":"machine_learning/artifical_neural_networks/index.html#artificial-neural-networks","text":"This note is about artificial neural networks (ANN). These networks belong to supervised learning and are used in the analysis and prediction of classification and regression problems.","title":"Artificial neural networks"},{"location":"machine_learning/convolutional_neural_networks/index.html","text":"Convolutional neural networks This note is about convolutional neural networks (CNN). These networks belong to supervised learning and are used in the field of computer vision.","title":"Convolutional neural networks"},{"location":"machine_learning/convolutional_neural_networks/index.html#convolutional-neural-networks","text":"This note is about convolutional neural networks (CNN). These networks belong to supervised learning and are used in the field of computer vision.","title":"Convolutional neural networks"},{"location":"machine_learning/recurrent_neural_networks/index.html","text":"Recurrent neural networks This note is about recurrent neural networks (RNN). These networks belong to supervised learning and are used in the analysis and prediction of time series or series like data. There are three general types of RNNs describing the series relation in input and output: Type Example One to many Sentence describing image Many to one Sentiment analysis Many to many Sentence translation Vanishing / exploding gradient problem RNNs can inhibit the vanishing / exploding gradient problem. One popular solution ist the use of long short-term memory (LSTM) nodes. Further readings: Diplomarbeit Josef Hochreiter, 1991: http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf","title":"Recurrent neural networks"},{"location":"machine_learning/recurrent_neural_networks/index.html#recurrent-neural-networks","text":"This note is about recurrent neural networks (RNN). These networks belong to supervised learning and are used in the analysis and prediction of time series or series like data. There are three general types of RNNs describing the series relation in input and output: Type Example One to many Sentence describing image Many to one Sentiment analysis Many to many Sentence translation","title":"Recurrent neural networks"},{"location":"machine_learning/recurrent_neural_networks/index.html#vanishing-exploding-gradient-problem","text":"RNNs can inhibit the vanishing / exploding gradient problem. One popular solution ist the use of long short-term memory (LSTM) nodes.","title":"Vanishing / exploding gradient problem"},{"location":"machine_learning/recurrent_neural_networks/index.html#further-readings","text":"Diplomarbeit Josef Hochreiter, 1991: http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf","title":"Further readings:"},{"location":"machine_learning/recurrent_neural_networks/ltsm.html","text":"Long short-term memory An LSTM is a RNN without the vanishing gradient problem. It introduces a cell state for. It is defined by \\[\\begin{eqnarray} f_t & = & \\sigma(W_f \\cdot x_t + U_f \\cdot h_{t-1} + b_f) \\\\ i_t & = & \\sigma(W_i \\cdot x_t + U_i \\cdot h_{t-1} + b_i) \\\\ o_t & = & \\sigma(W_o \\cdot x_t + U_o \\cdot h_{t-1} + b_o) \\\\ \\tilde{c}_t & = & \\tanh(W_c \\cdot x_t + U_c \\cdot h_{t-1} + b_c) \\\\ c_t & = & f_i \\circ c_t + i_i \\circ \\tilde{c}_t \\\\ h_t & = & o_t \\circ \\tanh(c_t) \\end{eqnarray}\\] There exists different flavors of the LSTM, e.g. peephole LSTM or LSTM without a forget gate.","title":"Long short-term memory"},{"location":"machine_learning/recurrent_neural_networks/ltsm.html#long-short-term-memory","text":"An LSTM is a RNN without the vanishing gradient problem. It introduces a cell state for. It is defined by \\[\\begin{eqnarray} f_t & = & \\sigma(W_f \\cdot x_t + U_f \\cdot h_{t-1} + b_f) \\\\ i_t & = & \\sigma(W_i \\cdot x_t + U_i \\cdot h_{t-1} + b_i) \\\\ o_t & = & \\sigma(W_o \\cdot x_t + U_o \\cdot h_{t-1} + b_o) \\\\ \\tilde{c}_t & = & \\tanh(W_c \\cdot x_t + U_c \\cdot h_{t-1} + b_c) \\\\ c_t & = & f_i \\circ c_t + i_i \\circ \\tilde{c}_t \\\\ h_t & = & o_t \\circ \\tanh(c_t) \\end{eqnarray}\\] There exists different flavors of the LSTM, e.g. peephole LSTM or LSTM without a forget gate.","title":"Long short-term memory"},{"location":"microservices/index.html","text":"Microservices","title":"Microservices"},{"location":"microservices/index.html#microservices","text":"","title":"Microservices"},{"location":"microservices/service_interaction.html","text":"Service interaction This note describes interaction procedures used in microservice architectures. Choreography In the choreography each service knows its role in a business process and therefore it needs to know all following services it has to call for the business process. sequenceDiagram participant Service1 participant Service2 participant Service3 Service1->>Service2: request Service1->>Service3: request Service3->>Service2: request Note : Response side suppressed for clarity. Orchestration In the orchestration a business process is executed by a central coordinator. The services theirself do not have to know the underlying business process and services involved in the business process. sequenceDiagram participant Coordinator participant Service1 participant Service2 participant Service3 Coordinator->>Service1: request Coordinator->>Service2: request Coordinator->>Service3: request Coordinator->>Service2: request Note : Response side suppressed for clarity. Another definition Choreography = Event-driven communication Orchestration = Command-driven communication","title":"Service interaction"},{"location":"microservices/service_interaction.html#service-interaction","text":"This note describes interaction procedures used in microservice architectures.","title":"Service interaction"},{"location":"microservices/service_interaction.html#choreography","text":"In the choreography each service knows its role in a business process and therefore it needs to know all following services it has to call for the business process. sequenceDiagram participant Service1 participant Service2 participant Service3 Service1->>Service2: request Service1->>Service3: request Service3->>Service2: request Note : Response side suppressed for clarity.","title":"Choreography"},{"location":"microservices/service_interaction.html#orchestration","text":"In the orchestration a business process is executed by a central coordinator. The services theirself do not have to know the underlying business process and services involved in the business process. sequenceDiagram participant Coordinator participant Service1 participant Service2 participant Service3 Coordinator->>Service1: request Coordinator->>Service2: request Coordinator->>Service3: request Coordinator->>Service2: request Note : Response side suppressed for clarity.","title":"Orchestration"},{"location":"microservices/service_interaction.html#another-definition","text":"Choreography = Event-driven communication Orchestration = Command-driven communication","title":"Another definition"},{"location":"testing/index.html","text":"Testing This note is about testing. Consumer-driven contract Links https://cucumber.io https://pact.io","title":"Testing"},{"location":"testing/index.html#testing","text":"This note is about testing.","title":"Testing"},{"location":"testing/index.html#consumer-driven-contract","text":"","title":"Consumer-driven contract"},{"location":"testing/index.html#links","text":"https://cucumber.io https://pact.io","title":"Links"}]}